---
title: "Econometrics Assignement 2- Brodard Lionel, Giro Tomas, Marchal Antoine, Schonenberger Ivan"
output: 
  html_notebook: default
---


------->  Please run this code with ' Run all' of *Ctrl+Alt+R*



# Question a) Generate the data: X, Y and epsilon

```{r}
## Parameters
alpha <- 0.5
beta_1 <- 0.8
beta_2 <- 1.3
real_beta <-rbind(alpha,beta_1,beta_2)
n <- 20
## Creation of the Matrix of X, the error term and calculation of Y
x_0 <- c(rep_len(1,n))
set.seed(12)
x_1 <- c(runif(n,0,10))
set.seed(1054)
x_2 <- c(runif(n,0,10))
set.seed(12)
X <- cbind(x_0,x_1,x_2)

eps <- c(rnorm(n,0,2))

Y <- alpha*x_0 + beta_1*x_1 + beta_2*x_2 + eps
message('X=')
round(X,1)
message('Y=')
round(Y,1)
message('epsilon=')
round(eps,1)
```


# Question b) Fit the model using OLS, estimate beta:
```{r}
est_B <- solve(t(X)%*%X)%*%t(X)%*%Y  # estimated beta
message('estimated beta=')
round(est_B,3)
```

# Question c) Check two equations

```{r}
est_Y <- X%*%est_B #estimated Y
est_e <- Y-est_Y #residual error
# creation of equations that need to be tested
message('equation 1:')
t(est_e)%*%est_e
message('=')
t(Y)%*%Y-t(est_B)%*%t(X)%*%Y-t(Y)%*%X%*%est_B+t(est_B)%*%t(X)%*%X%*%est_B
message('\nequation 2:')
t(est_B)%*%t(X)%*%Y
message('=')
t(Y)%*%X%*%est_B
```

# Question d) Check that the projection matrix P and the residual matrix M are symmetric and idempotent

```{r}
# compute matrix M 

M <- diag(20)-X%*%solve(t(X)%*%X)%*%t(X)

message('max(abs(MM-M)) =')
max(abs(M%*%M-M))

# compute matrix P -> P= Xinv(X'X)-X'
P <- X%*%solve(t(X)%*%X)%*%t(X)

message('\nmax(abs(PP-P)) =')
max(abs(P%*%P-P))

message('\nmax(abs(P-t(P))) =')
max(abs(P-t(P)))

message('\nmax(abs(M-t(M))) =')
max(abs(M-t(M)))

```
We can see that P and M are symmetric and idempotent.


#  Question e)  Using the same data sample, fit the simpler model which contains a constant and only the first regressor X1.  Compute its SSE and compare it with the SSE of the full model.
```{r}
# partial model: 
pX <- cbind(x_0,x_1) # pX stands for partial model X.
pest_B <- solve(t(pX)%*%pX)%*%t(pX)%*%Y
pest_Y <- pX%*%pest_B
pest_e <- Y-pest_Y
message('beta estimate of the partial model:')
pest_B
# SSE calculation
message('\nfull model SSE=')
fSSE <- t(est_e)%*%est_e
fSSE
message('\npartial model SSE=')
pSSE <- t(pest_e)%*%pest_e
pSSE

```
We can see that the sum of squared errors is bigger in the partial model. 
This is to be expected because it gives a worse fit of the data, at it has less degrees of freedom.


# Question f) Compute R2 and adjusted R2 of both models

```{r}
# R2 and ajusted R2 calculation
# SSR calculation est_Y-avY
# compute average of y
mean_Y <- c(rep_len(mean(Y),n))
# full model SSR 
fSSR <- sum((est_Y-mean_Y)^2)
# partial model SSR
pSSR <- sum((pest_Y-mean_Y)^2)
# SST calculation
# SST
SST <- sum((Y-mean_Y)^2)


# R squared
fR2 <- 1-(fSSE/SST)
pR2 <- 1-(pSSE/SST)



# Adjusted-R squared = 1 - ((1-R2)(n-1)/(n-k-1)), where k= number of independant regressor
fk <- 2 # regressors of full model
pk <- 1 # regressors of partial model
fAR2 <- 1-((1-fR2)*(n-1)/(n-fk-1)) # adjusted r2 of full model
pAR2 <- 1-((1-pR2)*(n-1)/(n-pk-1)) # adjusted r2 of partial model


T1<-data.frame('type'='full model','R2'=fR2,'adjusted R2'=fAR2)
T2<-data.frame('type'='partial model', 'R2'=pR2,'adjusted R2'=pAR2)
T3=rbind(T1,T2)
print(T3)
```

Table of the R2s of the two models

# Question g) Generate a sample of size n=500 for the full model. Fit the full model on subsamples of the first 10, 20, 50, 100, 250, 500 simulated observations, which gives you a sequence of 6 estimates.
```{r}
# create a vector n for the 6 subsamples
n=cbind(10,20,50,100,250,500)

# generate a sample of X, errors of n6=500 and calculate the full model
X_500_0 <- c(rep_len(1,500))
set.seed(12)
X_500_1 <- c(runif(500,0,10))
set.seed(1054)
X_500_2 <- c(runif(500,0,10))
X_500 <- cbind(X_500_0,X_500_1,X_500_2)
set.seed(12)
Neps <- c(rnorm(500,0,2))
Y_new <- c(rep_len(1,500))
Y_new <- alpha*X_500_0 + beta_1*X_500_1 + beta_2*X_500_2 + Neps

# create a loop to calculate the estimated beta for each subsample
estimated_beta_full <-matrix(1,nrow=3,ncol = 6)
ee <-c(rep(1,6))
for (i in 1:6){
  X<-cbind(rep(1,n[i]),X_500_1[1:n[i]],X_500_2[1:n[i]])
  estimated_beta_full[1:3,i]<-solve(t(X)%*%X)%*%t(X)%*%Y_new[1:n[i]]
  e<-Y_new[1:n[i]]-X%*%estimated_beta_full[1:3,i]
  ee[i]<-t(e)%*%e
}
# We create a data frame df1 with the values of the estimators
df1=data.frame('n'=t(n),'alpha'=round(estimated_beta_full[1,],2),'beta 1'=round(estimated_beta_full[2,],2),'beta 2'=round(estimated_beta_full[3,],2))

# generate a NEW sample of X, compute the new data frame and plot the evolutions
sec_X_500_0 <- c(rep_len(1,500))
set.seed(14)
sec_X_500_1 <- c(runif(500,0,10))
set.seed(27)
sec_X_500_2 <- c(runif(500,0,10))
sec_X_500 <- cbind(sec_X_500_0,sec_X_500_1,sec_X_500_2)
set.seed(14)
sec_Neps <- c(rnorm(500,0,2))
sec_Y_new <- c(rep_len(1,500))
sec_Y_new <- alpha*sec_X_500_0 + beta_1*sec_X_500_1 + beta_2*sec_X_500_2 + sec_Neps

# create a loop to calculate the estimated beta for each subsample
sec_estimated_beta_full <-matrix(1,nrow=3,ncol = 6)
sec_ee <-c(rep(1,6))
for (i in 1:6){
  X<-cbind(rep(1,n[i]),sec_X_500_1[1:n[i]],sec_X_500_2[1:n[i]])
  sec_estimated_beta_full[1:3,i]<-solve(t(X)%*%X)%*%t(X)%*%sec_Y_new[1:n[i]]
  e<-sec_Y_new[1:n[i]]-X%*%sec_estimated_beta_full[1:3,i]
  ee[i]<-t(e)%*%e
}



df2=data.frame('n'=t(n),'alpha'=round(sec_estimated_beta_full[1,],2),'beta 1'=round(sec_estimated_beta_full[2,],2),'beta 2'=round(sec_estimated_beta_full[3,],2))

#plots:

with(df1, 
     plot(n, 
          alpha, 
          type='l',
          ylim = c(min(c(df1$alpha,df2$alpha)),max(c(df1$alpha,df2$alpha)))))
lines(df2$n,
      df2$alpha, 
      col='red')
legend("topright",
       legend=c("first simulation", "second simulation"),
       col=c("black", "red"), lty=c(1,1))
title('Evolution of alpha')

with(df1, 
     plot(n, 
          beta.1, 
          type='l',
          ylim = c(min(c(df1$beta.1,df2$beta.1)),max(c(df1$beta.1,df2$beta.1)))))
lines(df2$n,df2$beta.1, col='red')
legend("topright",
       legend=c("first simulation", "second simulation"),
       col=c("black", "red"), lty=c(1,1))
title('Evolution of beta 1')
with(df1, 
     plot(n,
          beta.2, 
          type='l',
          ylim = c(min(c(df1$beta.2,df2$beta.2)),max(c(df1$beta.2,df2$beta.2)))))
lines(df2$n,df2$beta.2, col='red')
legend("bottomright",
       legend=c("first simulation", "second simulation"),
       col=c("black", "red"), lty=c(1,1))
title('Evolution of beta 2')

df1
df2

```

# How do the estimates evolve as the sample size increases?

We can see in the different graphs of the evolution of the parameters that they converge.

# Generate another sample of the same size n=500 and compute the resulting sequence of 6 estimates. 


# Do you obtain similar results?

As we can observe in the graphs we obtain similar results. Not only the evolutions converge, but they do to the same limits.
